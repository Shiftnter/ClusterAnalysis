# -*- coding: utf-8 -*-
"""ClusterMetrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kjkKT2lAnHqo5OOIEH1H9DAkkS20krrT
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, Birch, DBSCAN
from sklearn.metrics import silhouette_score, adjusted_rand_score, rand_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances
from collections import Counter
from sklearn.datasets import make_blobs

from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
data = pd.read_csv('/content/Wholesale customers data.csv')
X = data[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']].values

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize clustering algorithms
kmeans = KMeans(n_clusters=3, random_state=42)
birch = Birch(n_clusters=3)
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Fit the models
kmeans_labels = kmeans.fit_predict(X_scaled)
birch_labels = birch.fit_predict(X_scaled)
dbscan_labels = dbscan.fit_predict(X_scaled)

def dunn_index(labels, X):
    unique_labels = np.unique(labels)
    if len(unique_labels) < 2:
        return -1  # Dunn's index is not defined for 1 cluster

    inter_cluster_distances = []
    intra_cluster_distances = []

    for label in unique_labels:
        cluster_points = X[labels == label]
        if len(cluster_points) < 2:
            continue
        intra_dist = np.mean(pairwise_distances(cluster_points))
        intra_cluster_distances.append(intra_dist)

        for other_label in unique_labels:
            if label != other_label:
                other_cluster_points = X[labels == other_label]
                inter_dist = np.mean(pairwise_distances(cluster_points, other_cluster_points))
                inter_cluster_distances.append(inter_dist)

    return min(inter_cluster_distances) / max(intra_cluster_distances) if intra_cluster_distances else -1

def purity_score(labels, true_labels):
    if true_labels is None:
        return None  # Purity requires true labels
    contingency_matrix = pd.crosstab(true_labels, labels)
    return np.sum(np.amax(contingency_matrix.values, axis=0)) / np.sum(contingency_matrix.values)

# Evaluation Metrics
def evaluate_clustering(labels, X, true_labels=None):
    metrics = {}
    metrics['Silhouette Index'] = silhouette_score(X, labels) if len(set(labels)) > 1 else -1
    metrics['Dunn\'s Index'] = dunn_index(labels, X)
    metrics['Rand Index'] = rand_score(true_labels, labels) if true_labels is not None else None
    metrics['Adjusted Rand Index'] = adjusted_rand_score(true_labels, labels) if true_labels is not None else None
    metrics['Purity'] = purity_score(labels, true_labels)
    return metrics

kmeans_metrics = evaluate_clustering(kmeans_labels, X_scaled, dbscan_labels)
print("**** K-Means Evaluation Metrics ****")
for i in kmeans_metrics:
    print(f"{i}: {kmeans_metrics[i]}")

birch_metrics = evaluate_clustering(birch_labels, X_scaled, kmeans_labels)
print("**** Birch Evaluation Metrics ****")
for i in birch_metrics:
    print(f"{i}: {birch_metrics[i]}")

dbscan_metrics = evaluate_clustering(dbscan_labels, X_scaled, kmeans_labels)
print("**** DBSCAN Evaluation Metrics ****")
for i in dbscan_metrics:
    print(f"{i}: {dbscan_metrics[i]}")

"""***Data*** **Visualization**"""

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

def plot_clustering(X, labels, title):
    plt.figure(figsize=(6, 4))
    unique_labels = np.unique(labels)
    colors = plt.cm.get_cmap('viridis', len(unique_labels))  # Use a colormap

    for i, label in enumerate(unique_labels):
        plt.scatter(X[labels == label, 0], X[labels == label, 1],
                    color=colors(i), label=f'Cluster {label}', alpha=0.6)

    plt.title(title)
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.legend()
    plt.grid()
    plt.show()

# Visualize K-Means clustering
plot_clustering(X_pca, kmeans_labels, 'K-Means Clustering (After PCA Reduction)')

# Visualize Birch clustering
plot_clustering(X_scaled, birch_labels, 'Birch Clustering (After PCA Reduction)')

# Visualize DBSCAN clustering
plot_clustering(X_scaled, dbscan_labels, 'DBSCAN Clustering (After PCA Reduction)')

"""# Yale Datset"""

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Set the path to the directory containing the images
data_dir = '/content/drive/MyDrive/Yale_dataset/68'
image_files = os.listdir(data_dir)

# Display a few images
plt.figure(figsize=(10, 10))
for i, image_file in enumerate(image_files[:9]):  # Display the first 9 images
    img = mpimg.imread(os.path.join(data_dir, image_file))
    plt.subplot(3, 3, i + 1)
    plt.imshow(img, cmap='gray')
    plt.axis('off')
    plt.title(image_file)

plt.show()

from PIL import Image
def load_and_preprocess_images(data_dir):
    images = []
    image_names = []
    for image_file in os.listdir(data_dir):
        # Load the image using PIL
        img = Image.open(os.path.join(data_dir, image_file)).convert('L')  # Convert to grayscale
        img_resized = img.resize((100, 100))  # Resize to 100x100 pixels
        images.append(np.array(img_resized).flatten())  # Flatten the image to a vector
        image_names.append(image_file)
    return np.array(images), image_names

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Set the path to the directory containing the images
data_dir = '/content/drive/MyDrive/Yale_dataset/68'

# Load and preprocess images
images, image_names = load_and_preprocess_images(data_dir)

# Standardize the data
scaler = StandardScaler()
images_scaled = scaler.fit_transform(images)

# Run K-Means clustering
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans_labels = kmeans.fit_predict(images_scaled)

birch = Birch(n_clusters=n_clusters)
birch_labels = birch.fit_predict(images_scaled)

from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt

k = 5  # This should be equal to min_samples
neighbors = NearestNeighbors(n_neighbors=k)
neighbors_fit = neighbors.fit(images_scaled)
distances, indices = neighbors_fit.kneighbors(images_scaled)

# Sort distances to find the optimal eps
distances = np.sort(distances[:, k-1], axis=0)

# Plot the k-distance graph
plt.figure(figsize=(6, 4))
plt.plot(distances)
plt.title('K-Distance Graph')
plt.xlabel('Points sorted by distance')
plt.ylabel('Distance to {}th nearest neighbor'.format(k))
plt.grid()
plt.show()

dbscan = DBSCAN(eps=108, min_samples=5)
dbscan_labels = dbscan.fit_predict(images_scaled)

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score

# Calculate clustering metrics for Kmeans
silhouette = silhouette_score(images_scaled, kmeans_labels)
davies_bouldin = davies_bouldin_score(images_scaled, kmeans_labels)
calinski_harabasz = calinski_harabasz_score(images_scaled, kmeans_labels)
inertia = kmeans.inertia_
ari = adjusted_rand_score(birch_labels, kmeans_labels)
print('****K-Means Clustering Metrics****')
print(f'Silhouette Score: {silhouette}')
print(f'Davies-Bouldin Score: {davies_bouldin}')
print(f'Calinski-Harabasz Score: {calinski_harabasz}')
print(f'Inertia: {inertia}')
print(f'Adjusted Rand Index: {ari}')

# Calculate clustering metrics for Birch
silhouette = silhouette_score(images_scaled, birch_labels)
davies_bouldin = davies_bouldin_score(images_scaled, birch_labels)
calinski_harabasz = calinski_harabasz_score(images_scaled,birch_labels)
ari = adjusted_rand_score(kmeans_labels, birch_labels)
print('****Birch Clustering Metrics****')
print(f'Silhouette Score: {silhouette}')
print(f'Davies-Bouldin Score: {davies_bouldin}')
print(f'Calinski-Harabasz Score: {calinski_harabasz}')
print(f'Adjusted Rand Index: {ari}')

"""***Data Visualize***  (note - Only the first image from each cluster is displayed.)"""

def visualize_clusters(images, predicted_labels, n_clusters):
    plt.figure(figsize=(15, 10))
    for cluster in range(n_clusters):
        plt.subplot(1, n_clusters, cluster + 1)
        cluster_images = images[predicted_labels == cluster]
        if cluster_images.size > 0:
            # Display the first image from the cluster
            plt.imshow(cluster_images[0].reshape(100, 100), cmap='gray')
            plt.axis('off')
            plt.title(f'Cluster {cluster}')
    plt.show()

visualize_clusters(images, birch_labels, n_clusters)
print(birch_labels)

def visualize_dbscan_clusters(images, predicted_labels):
    unique_labels = set(predicted_labels)
    print(unique_labels)
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)

    plt.figure(figsize=(15, 10))
    for cluster in unique_labels:
        if cluster == -1:
            # Noise points
            continue
        cluster_images = images[predicted_labels == cluster]
        if cluster_images.size > 0:
            # Display the first image from the cluster
            plt.subplot(1, n_clusters, cluster + 1)
            plt.imshow(cluster_images[0].reshape(100, 100), cmap='gray')
            plt.axis('off')
            plt.title(f'Cluster {cluster}')

    plt.show()

visualize_dbscan_clusters(images, dbscan_labels)
print(dbscan_labels)

visualize_clusters(images, kmeans_labels, n_clusters)
print(kmeans_labels)

"""the code below helps to detmine the eps val for DBSCAN algo"""

!pip install kneed

from kneed import KneeLocator

# Use KneeLocator to find the knee point
kneedle = KneeLocator(range(len(distances)), distances, curve='convex', direction='increasing')
knee_index = kneedle.elbow
eps_value = distances[knee_index]

print(f"Suggested eps value from knee detection: {eps_value}")

from google.colab import drive
drive.flush_and_unmount()